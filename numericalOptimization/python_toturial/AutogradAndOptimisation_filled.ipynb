{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8842508c-dc85-4225-9064-5c017ed25494",
   "metadata": {},
   "source": [
    "# Autograd and Optimisation\n",
    "\n",
    "Almost every scientific area, especially experimental work requires function fitting, for example to find the parameters for a theory that describe the data best. One classical example is linear regression, which we discussed in the Assignment 6 on day 2. Here we assume that two sets of data x, and y are related by a linear function, $f(x)=w^Tx$. Of course most datasets are much more complex. While for many standard models there are already packages that automate packages (for example sklearn for standard machine learning models, which we discuss later), you are often on your own if your goal is to adapt or change a model. \n",
    "\n",
    "We will now talk about model fitting in a more general context: most fitting approaches find a set of parameters that minimize some measure of error, for example the expected squared difference between model prediction and real data, mean squared error. We will therefore start first with the question: how can we find a (local) minimum of an arbitrary function $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f55cf02-dde9-4603-975b-9663afc7988a",
   "metadata": {},
   "source": [
    "Scipy is a scientific package that includes tools for a lot of scientific numeric purposes, including a package for optimisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb810ee6-f00f-418e-b035-e669d721b70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\huste\\documents\\github\\.venv\\lib\\site-packages (1.14.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in c:\\users\\huste\\documents\\github\\.venv\\lib\\site-packages (from scipy) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b50643-0adc-4e00-8fd0-11910c7b20e4",
   "metadata": {},
   "source": [
    "We will use the subpackage scipy.optimize \n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/optimize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f127a09e-8116-4fd9-b1df-d3e842164721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb5880-1e3e-43f6-942b-2a019abd2ec4",
   "metadata": {},
   "source": [
    "## Functions with several return values: defining an optimisation problem\n",
    "\n",
    "We want to find a minimum of a simple function $f(x)=1/2 x^Tx$\n",
    "\n",
    "where $x$ is a n dimensional vector. Efficient optimisation requires the knowledge of the gradient of the function, i.e., the partial derivatives of $f$ wrt each parameter. for our function $f$ the gradient is $g(x)=x$. For scipy.minimize, we need to return both the result f and the gradient g from the same function call, we will introduce this in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e6e8d0-378c-4c01-b9ce-a987b31711d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def problem(x):\n",
    "    #compute the value f:\n",
    "    f=0.5 * np.inner(x,x) #np.inner(x,y) ciomputes x^Ty you could also implement this as np.sum(x**2)\n",
    "    #compute the gradient g\n",
    "    g = x \n",
    "\n",
    "    #returning several values returns can be done with a simple comma separated list. This makes the function return a tuple (f,g)\n",
    "    return f,g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f27ebcdf-6351-46f1-9ca2-19ef9431bd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.0, array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones(10)\n",
    "result = problem(x)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7f91ba8-2a5a-4fbc-b3f9-16c768336cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#python allows a short cut: we can assign a tuple to a comma separated list of exactly the same size:\n",
    "f,g = result\n",
    "print(f)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b8d0471-080b-42ad-970a-1fa896d36d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#and we can do this in one line. we saw this already on day 2 with the function np.linalg.eigh which returned eigenvalues and eigenvectors\n",
    "\n",
    "f,g = problem(x)\n",
    "print(f)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a84a6-3767-45c2-85da-e3f5ffaff442",
   "metadata": {},
   "source": [
    "We now want to minimize the function using scipy.optimize.minimize.\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize\n",
    "\n",
    "We make use of another cool feature of python: functions can take another function as argument.  We use that to tell minimize that it should try to find the minimum of our defined function \"problem\". The only thing we need to tell it is an initial guess for a solution: the starting point of the optimisation as well as tell it that our problem returns a gradient (called jacobian or jac in the minimisation package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f60bc73d-ea9b-4b40-b29a-2fad21e28c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init=np.array([1,2,3,4])\n",
    "optimResult = optim.minimize(problem,x_init, jac=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6478e897-12c8-4dfc-8456-500a003e9d9e",
   "metadata": {},
   "source": [
    "The result object is a class object which stores mostly some interesting data about the final result of the optimisation\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html#scipy.optimize.OptimizeResult\n",
    "\n",
    "We can print out the result object and it provides some (more or less) valueable information:\n",
    "- success: Was the optimisation successful?\n",
    "- message: a human readable message for the error status\n",
    "- status: a computer interpretable sgtatus code for the message\n",
    "- x: the final guess for the minimum\n",
    "- fun: the final achieved function value, the value of at problem(x)\n",
    "- nit/nfev: the number of iterations/function evaluations the algorithm took\n",
    "- jac: the value of g returned at problem(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab29507e-ac6b-46e7-9c4e-02ba3a3300e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 8.16594296420188e-32\n",
       "        x: [-5.551e-17 -3.331e-16 -2.220e-16  0.000e+00]\n",
       "      nit: 3\n",
       "      jac: [-5.551e-17 -3.331e-16 -2.220e-16  0.000e+00]\n",
       " hess_inv: [[ 1.000e+00 -1.388e-17  1.388e-17  5.551e-17]\n",
       "            [-1.388e-17  1.000e+00  2.776e-17  5.551e-17]\n",
       "            [ 1.388e-17  8.327e-17  1.000e+00  5.551e-17]\n",
       "            [ 2.776e-17  0.000e+00  1.110e-16  1.000e+00]]\n",
       "     nfev: 4\n",
       "     njev: 4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d457dfc-5ff6-4cf5-a3e7-d5a46bdea2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.16594296420188e-32,\n",
       " array([-5.55111512e-17, -3.33066907e-16, -2.22044605e-16,  0.00000000e+00]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xOpt = optimResult.x\n",
    "\n",
    "problem(xOpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24e694d-9bbc-4a57-949f-47cc3e01699b",
   "metadata": {},
   "source": [
    "# Autodifferentiation\n",
    "\n",
    "Finding the gradient of a function is error prone. For this reason, and due to the need of automating this for the big models in machine learning, there are a large number of libraries that implement automatic differentiation. The most well known packages are pytorch, tensorflow and jax. But those libraries are so large and complex(for example they also offer full gpu support) that we cannot introduce them here. They would be part of a proper machine learning course.\n",
    "\n",
    "instead we use a smaller package called autograd, which is just \"numpy with gradients\". \n",
    "\n",
    "https://github.com/HIPS/autograd\n",
    "\n",
    "#the next lines installs autograd with support for scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a72b96ed-f2a9-48aa-9946-1c4056d9cf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autograd[scipy]\n",
      "  Downloading autograd-1.7.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\huste\\documents\\github\\.venv\\lib\\site-packages (from autograd[scipy]) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\huste\\documents\\github\\.venv\\lib\\site-packages (from autograd[scipy]) (1.14.1)\n",
      "Downloading autograd-1.7.0-py3-none-any.whl (52 kB)\n",
      "Installing collected packages: autograd\n",
      "Successfully installed autograd-1.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install autograd[scipy]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d4a343-ee9e-43f1-ab30-16f8443bac54",
   "metadata": {},
   "source": [
    "Before you continue, please restart the kernel! We do not want normal numpy to interfere with what we do next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4afe9-b50a-4a3a-acf6-0e2c2c314a4f",
   "metadata": {},
   "source": [
    "Autograd comes with its own version of numpy and a small set of tools that interact with this version of numpy. It supports almost everything that normal numpy supports with only a few exceptions. e..g, if you have a function with argument x and you want to compute the derivative wrt x, then you cannot use indexing to assign new values to x inside the function. But otherwise everything works, for example even +=,-= etc.\n",
    "\n",
    "We use this to implement linear regression as in assignment 6.2 on day 2. And we use the mean squared error function from task 7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e91c9cf-8a58-4c07-96df-6406a3115024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np # please see that we are using the autograd numpy package now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1249b2-48ae-432f-809a-21a6f87ab619",
   "metadata": {},
   "source": [
    "First we implement our model class $model(x,w)$. We interpret this as: the prediction of a model with parameters w given a new data input x.\n",
    "For linear regression:  $model(x,w)=w^Tx$. \n",
    "\n",
    "We implement it to support batching, i.e., computing the result for not a single vector x but a matrix X where each row is a data point.\n",
    "In this case: $model(X,w)=Xw$. We call this a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33bc20d5-bc7b-4e2d-95a2-f6e948a70e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(X,w):\n",
    "    return X@w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04431a0-3a5a-48da-9e30-b2b13507ee94",
   "metadata": {},
   "source": [
    "Now we implement the mean squared error $mse=mean[(model(x,w)-y)^2]$. \n",
    "To make this reusable, we will take $f$ as function argument, like in minimize before!\n",
    "Parameters of mse:\n",
    "- X the matrix of points\n",
    "- y a vector of ground truth labels (our measurement data)\n",
    "- model the model class we want to use for the prediction\n",
    "- w the parameters of f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07faa6ee-44af-4650-b203-e29656137729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(X,y,model,w):\n",
    "    prediction = model(X,w) #our model prediction for each point in X\n",
    "    errors = (prediction - y)**2 #(f(x,w)-y)^2 for all our data points\n",
    "    return np.mean(errors) #return the average of all errors. note that here np is no longer numpy.mean but autograd.numpy.mean!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c01802-7405-47e5-a4c8-eb61fb9342a3",
   "metadata": {},
   "source": [
    "We now generate a problem using the same code as in assignment 6.2 day 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8bb1a7f-f561-46b5-860c-fad17361f305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# A little help for X.\n",
    "# We want to sample a simple 1D problem with N=200 points.\n",
    "# we add a second column that is constant 1,which allows us to learn a constant offset parameter. \n",
    "# Thus we actually compute f(x)=w[0]x+w[1]\n",
    "N=200\n",
    "X = 10*np.random.randn(N,2) #N points with 2 values each\n",
    "X[:,-1] = 1 #overwrite the last column (using -1 here instead of \"1\" allows us to change the dimensionality of the data!)\n",
    "w_truth = np.array([2.0,3.0]) #the true unknown values\n",
    "#Subtask 1: compute y\n",
    "# for this first generate a normal distributed vector epsilon with N entries\n",
    "# then compute y_i= w^Tx_i+epsilon_i\n",
    "epsilon= np.random.randn(N)\n",
    "y = linear_model(X,w_truth)+epsilon\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d252beb-d5e6-448b-aac3-18935aa28038",
   "metadata": {},
   "source": [
    "Let us start computing the mean squared error using the values of w_truth used to generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9568082f-4c1b-4feb-a2ba-e93ce075a957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9968248895442396"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(X,y,linear_model, w_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba087a0-5ac8-499f-9c91-6cdfe0c7c454",
   "metadata": {},
   "source": [
    "Now we want to minimize. For this, we need to compute our gradient. Introduce the key tool of autograd:\n",
    "The function autograd.value_and_grad takes a function f(x) and creates a new problem that returns a tuple: f(x) and g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87aac90a-8a6f-45fc-8267-c81273c83651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9968248895442396\n",
      "[0.83225316 0.13791519]\n"
     ]
    }
   ],
   "source": [
    "from autograd import value_and_grad\n",
    "\n",
    "#for our optimiosation problem, we need to create a function that only depends on the parameters w.\n",
    "def problem_f(w):\n",
    "    #note that the variables X,y, and the function linear_model that we defined above are also accessible INSIDE f!\n",
    "    #so we can just call mse with the first three arguments as before!\n",
    "    return mse(X,y,linear_model, w) \n",
    "\n",
    "#the problem we want to minimize!\n",
    "problem = value_and_grad(problem_f)\n",
    "\n",
    "#let us try it out by computing value and gradient at the true value\n",
    "f,g = problem(w_truth)\n",
    "print(f)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aba907f0-1cac-409b-91a3-eb3397b70c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "w_init = np.ones(2)\n",
    "optimResult = minimize(problem, w_init,jac=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c042f1e-2fe0-466a-9ae1-46fd8a4a1c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 0.9904571669423347\n",
       "        x: [ 1.996e+00  2.933e+00]\n",
       "      nit: 4\n",
       "      jac: [ 1.130e-14  6.054e-16]\n",
       " hess_inv: [[ 5.301e-03 -2.008e-03]\n",
       "            [-2.008e-03  5.008e-01]]\n",
       "     nfev: 6\n",
       "     njev: 6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimResult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8db42-6271-49f0-ba01-0d9fe069d614",
   "metadata": {},
   "source": [
    "# Challenge Autodiff, and Classes (harder to HARD)\n",
    "\n",
    "We would like to have our code easier to understand and more modular. For example, right now mse needs to take 4 arguments, X,y,model and w. \n",
    "We can use classes to make this possible and order our code so that mse does not even need to know that f has arguments!\n",
    "\n",
    "Check the tutorial on classes and read the python docs on classes as needed. We want to implement a new set of classes where each model follows a specific interface. \n",
    "\n",
    "- \\_\\_init\\_\\_: initializes the parameters of the model so that can it be used. This means often that init needs to take a few parameters which say how big internal arrays should be, for example what the dimensionality of the data is we want to use the model for.\n",
    "- w=model.getParameters(): a function returning a vector that appends all parameters of the model to a single array\n",
    "- model.setParameters(w): a function that sets all parameters of the model. The idea is that if we have two models a nd use getParameters on the one model and then use setParameters(w) on the other model, they will afterwise produce the same predictions.\n",
    "- model.predict(X): Returns the predictions of the model for input X using the currently set parameters. you can also instead rename the function ton  \\_\\_call_\\_\\_ in which case you can write model(X) insteads of model.predict(X).\n",
    "\n",
    "\n",
    "Implement a new class LinearModel that follows that interface. It is supposed to have one data attribute\n",
    "x.w which is a numpy array that holds the linear parameters. This means:\n",
    "\n",
    "- \\_\\_init\\_\\_: Takes one parameter n and adds model.w as a random normal distributed vector of length n.\n",
    "- model.getParameters(): returns model.w\n",
    "- model.setParameters(w): sets model.w to the new value.\n",
    "- model.predict(X): X@w\n",
    "\n",
    "Implement a new function mse which takes a model and returns the mean squared error.\n",
    "Then implement a new problem_f method which uses first model.setParameters to set the model to the new parameter values that are provided as argument and then computes mse for the model and data. Then fit the model parameters using minimize. As starting point you can use the value returned by model.getParameters().\n",
    "\n",
    "\n",
    "(HARD) You can then challenge yourself by implementing a more complex application, for example a neural network with a hidden layer with K neurons:\n",
    "\n",
    "$f(x)=b_2+w^T_2 h(W_1x+b_1)$\n",
    "\n",
    "here, h is a elementwise nonlinear function, like h(x)=np.maximum(x,0) (the ReLU activation function in machine learning). $W_1$ is a matrix with K rows and number of columns the same as the dimensions of the inoput points x. $w_2,b_1$ are vectors of length $K$ and $b_2$ is a scalar. These 4 objects make up the parameters of the neural network. To implement setParameters you can use slicing, e.g., model.w_2=w\\[0:K\\]  to set w_2 to the first K values in w. you should also look up np.reshape to transform a vector into a matrix with the same number of elements (e.g., to turn the sliced parameters in w into the matrix W_1. For getParameters() you can use W_2.flatten() to turn a matrix into a vector and use np.concatenate to create a vector w from the 4 components.\n",
    "\n",
    "then get ayour favourite dataset (or make something up) and try whether you can get it to work!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a184bd0f-6cb5-4694-b5fb-225aa48d475a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
